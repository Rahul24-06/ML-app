# ML-app

# A. Problem statement
# B. Dataset description
# C. Models used: 

### Comparison Table with the evaluation metrics

| ML Model Name | Accuracy | AUC | Precision | Recall | F1 | MCC |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
| **Logistic Regression** | | | | | | |
| **Decision Tree** | | | | | | |
| **kNN** | | | | | | |
| **Naive Bayes** | | | | | | |
| **Random Forest (Ensemble)** | | | | | | |
| **XGBoost (Ensemble)** | | | | | | |


### Model Performance Observations

| ML Model Name | Observation about model performance |
| :--- | :--- |
| **Logistic Regression** | Baseline model; performed well on linear features but struggled with non-linear relationships. |
| **Decision Tree** | High interpretability, though prone to overfitting without proper pruning. |
| **kNN** | Performance was highly sensitive to the choice of 'k' and feature scaling. |
| **Naive Bayes** | Extremely fast to train; performed surprisingly well despite the 'independence' assumption. |
| **Random Forest (Ensemble)** | Significantly reduced variance compared to a single Decision Tree; very robust. |
| **XGBoost (Ensemble)** | Achieved the highest overall metrics after hyperparameter tuning. |
